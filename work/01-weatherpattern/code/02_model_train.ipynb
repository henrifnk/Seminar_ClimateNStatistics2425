{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7415b322-ddcc-44b5-acf2-f5505b0b38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl \n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['savefig.dpi'] = 400\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "# os.environ[\"NUMBA_NUM_THREADS\"] = \"1\"\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.dirname(os.path.abspath('__file__')) # os.path.abspath('')\n",
    "data_path = os.path.join(dir, '../data/raw')\n",
    "pro_data_path = os.path.join(data_path, '../processed')\n",
    "res_path = os.path.join(dir, '../results')\n",
    "if not os.path.exists(res_path):\n",
    "    os.makedirs(res_path)\n",
    "train_path = os.path.join(res_path, \"train\")\n",
    "if not os.path.exists(train_path):\n",
    "    os.makedirs(train_path)\n",
    "\n",
    "years = [str(y) for y in range(1950, 1981)]  # Match paper's 1950-1980 period\n",
    "months = [f\"{m:02d}\" for m in range(1, 13)]  # All 12 months\n",
    "days = [f\"{d:02d}\" for d in range(1, 32)]  # 1-31 days\n",
    "times = [\"00:00\"]  # Daily data at midnight\n",
    "\n",
    "# Define region [N, W, S, E]\n",
    "area = [75, -65, 30, 45]  # Matches the paper's study region\n",
    "\n",
    "slp_path = os.path.join(data_path, 'slp')\n",
    "z500_path = os.path.join(data_path, 'z500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78236aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 labels:\n",
      "{'HM', 'HNFA', 'HFZ', 'WW', 'BM', 'TRW', 'SEZ', 'SWZ', 'HNZ', 'NWZ', 'TRM', 'HNA', 'NEA', 'HB', 'SZ', 'WZ', 'SWA', 'HFA', 'SA', 'NEZ', 'NA', 'NWA', 'TM', 'WA', 'NZ', 'SEA', 'WS', 'HNFZ', 'TB'}\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "X = np.load(os.path.join(pro_data_path, \"clean_2channel_data.npy\"))  # Shape: (time, 2, lat, lon)\n",
    "X_train = X.copy()\n",
    "labels = np.load(os.path.join(pro_data_path, \"clean_gwl_1950_1980_lbl.npy\"))\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "print(f\"{num_classes} labels:\")\n",
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb8fc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Class Indices: [ 4 13 13 ... 25 25 25]\n",
      "One-hot Encoded Labels: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Encode class values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_indices = label_encoder.fit_transform(labels)\n",
    "train_idx = np.arange(len(y_indices))\n",
    "y_indices_train = y_indices[train_idx]\n",
    "if sum(y_indices_train != y_indices) > 0:\n",
    "    raise ValueError(\"Number of labels does not match the number of samples\")\n",
    "np.save(os.path.join(train_path, \"train_idx.npy\"), train_idx)\n",
    "\n",
    "lbl_2_idx = {label: i for i, label in zip(range(len(label_encoder.classes_)), label_encoder.classes_)}\n",
    "idx_2_lbl = {i: label for label, i in lbl_2_idx.items()}\n",
    "np.save(os.path.join(pro_data_path, \"lbl_2_idx.npy\"), lbl_2_idx)\n",
    "np.save(os.path.join(pro_data_path, \"idx_2_lbl.npy\"), idx_2_lbl)\n",
    "print(\"Encoded Class Indices:\", y_indices)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_tensor = torch.tensor(y_indices, dtype=torch.long)\n",
    "y_one_hot = F.one_hot(y_tensor, num_classes=num_classes).float()\n",
    "y_train = y_one_hot[train_idx,:]\n",
    "print(\"One-hot Encoded Labels:\", y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fde17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8946, 2237)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_time = np.load(os.path.join(pro_data_path, \"clean_time_1950_1980.npy\"))\n",
    "valid_time_train = valid_time[train_idx]\n",
    "valid_time_train = pd.to_datetime(valid_time_train)\n",
    "valid_time_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92966e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_variance(X_train, train_path):\n",
    "    \"\"\"\n",
    "    Compute mean and variance for standardization (Z-score normalization).\n",
    "\n",
    "    :param X_train: Training data of shape (time, 2, lat, lon)\n",
    "    :return: Mean and variance of shape (2, lat, lon)\n",
    "    \"\"\"\n",
    "    mean_tr = np.mean(X_train, axis=0)  # Compute per-channel mean\n",
    "    var_tr = np.var(X_train, axis=0)  # Compute per-channel variance\n",
    "    np.save(os.path.join(train_path, \"mean_tr.npy\"), mean_tr)\n",
    "    np.save(os.path.join(train_path, \"var_tr.npy\"), var_tr)\n",
    "    return mean_tr, var_tr\n",
    "\n",
    "\n",
    "def normalize_with_moments(X, mean_tr, var_tr, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Apply Z-score normalization to new data.\n",
    "\n",
    "    :param X: NumPy array of shape (time, 2, lat, lon)\n",
    "    :param mean_X: Mean of training set (2, lat, lon)\n",
    "    :param var_X: Variance of training set (2, lat, lon)\n",
    "    :param epsilon: Small value to prevent division by zero\n",
    "    :return: Standardized X\n",
    "    \"\"\"\n",
    "    return (X - mean_tr) / np.sqrt(var_tr + epsilon)  # Element-wise normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "mean_tr, var_tr = compute_mean_variance(X_train, train_path)\n",
    "\n",
    "X_train_norm = normalize_with_moments(X_train, mean_tr, var_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0081f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_seasons(X, valid_month, mean_train_seasonal):\n",
    "    \"\"\"\n",
    "    Perform seasonal adjustment by subtracting the seasonal mean.\n",
    "\n",
    "    :param X: Data array of shape (time, 2, lat, lon)\n",
    "    :param valid_month: List of months of timestamps corresponding to each data sample\n",
    "    :param mean_train_seasonal: Array of seasonal means (12, 2, lat, lon)\n",
    "    :return: Seasonally adjusted data\n",
    "    \"\"\"\n",
    "    X_adjusted = np.zeros_like(X, dtype=X.dtype)\n",
    "\n",
    "    season_dict = {\n",
    "        \"Spring\": [3, 4, 5],   # Mar, Apr, May\n",
    "        \"Summer\": [6, 7, 8],   # Jun, Jul, Aug\n",
    "        \"Fall\":   [9, 10, 11],  # Sep, Oct, Nov\n",
    "        \"Winter\": [12, 1, 2]  # Dec, Jan, Feb\n",
    "    }\n",
    "\n",
    "    for i, months in enumerate(season_dict.values()):\n",
    "        season_indices = np.isin(valid_month, months)\n",
    "        X_adjusted[season_indices,:,:,:] = X[season_indices,:,:,:] - mean_train_seasonal[i]\n",
    "\n",
    "    return X_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70311cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal adjustment\n",
    "valid_month_train = np.array([t.month for t in valid_time_train])\n",
    "\n",
    "season_dict = {\n",
    "        \"Spring\": [3, 4, 5],   # Mar, Apr, May\n",
    "        \"Summer\": [6, 7, 8],   # Jun, Jul, Aug\n",
    "        \"Fall\":   [9, 10, 11],  # Sep, Oct, Nov\n",
    "        \"Winter\": [12, 1, 2]  # Dec, Jan, Feb\n",
    "    }\n",
    "\n",
    "mean_train_seasonal = np.zeros((4, X_train_norm.shape[1], X_train_norm.shape[2], X_train_norm.shape[3]))\n",
    "\n",
    "for i, months in enumerate(season_dict.values()):\n",
    "    season_indices = np.isin(valid_month_train, months)\n",
    "    mean_train_seasonal[i] = np.mean(X_train_norm[season_indices,:,:,:], axis=0)\n",
    "\n",
    "np.save(os.path.join(train_path, \"mean_train_seasonal.npy\"), mean_train_seasonal)\n",
    "train_path = os.path.join(res_path, \"train\")\n",
    "X_train_seasonal = center_seasons(X_train_norm, valid_month_train, mean_train_seasonal)\n",
    "np.save(os.path.join(train_path, \"X_train_seasonal.npy\"), X_train_seasonal)\n",
    "np.save(os.path.join(train_path, \"y_train.npy\"), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ff641-ab7d-4959-a6cf-1d36f4a0994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import torch.nn as nn\n",
    "\n",
    "class CirculationTypeClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, lat, lon, dropout_rate=0.3, out_channels1=8, out_channels2=16, kernel_size=5, fc1_size=50):  # 2 channels (SLP & Z500)\n",
    "        super(CirculationTypeClassifier, self).__init__()\n",
    "\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        padding = kernel_size // 2\n",
    "      \n",
    "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=out_channels1, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels1, out_channels=out_channels2, kernel_size=kernel_size, stride=1, padding=padding)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=1)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(self._calculate_fc_input_size(), fc1_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(fc1_size, num_classes)\n",
    "        \n",
    "    def _calculate_fc_input_size(self):\n",
    "        # Helper function to calculate the input size for the first fully connected layer\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 2, self.lat, self.lon)\n",
    "            x = self.pool1(F.relu(self.conv1(x)))\n",
    "            x = self.pool2(F.relu(self.conv2(x)))\n",
    "            x = self.flatten(x)\n",
    "            return x.shape[1]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        x = F.relu(self.conv1(x))  # Assuming ReLU activation\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))  # Assuming ReLU activation\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))  # Assuming ReLU activation\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Assuming no activation function for the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4db692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import optuna\n",
    "import json\n",
    "\n",
    "# Define cross-validation strategy\n",
    "outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Set ensemble number (30 models as paper)\n",
    "ensemble_size = 30\n",
    "ensemble_per_fold = ensemble_size // outer_cv.get_n_splits()\n",
    "\n",
    "# Data dimensions\n",
    "time_dim = X.shape[0]\n",
    "n_vars = X.shape[1]\n",
    "lat = X.shape[2]\n",
    "lon = X.shape[3]\n",
    "\n",
    "epochs = 35 # 35 as paper\n",
    "patience = 5 # 5 as paper\n",
    "batch_size = 128 # 128 as paper\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = os.path.join(res_path, \"models\")\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "best_models = []  # Store best models for ensemble\n",
    "\n",
    "\n",
    "X_train_seasonal = torch.tensor(X_train_seasonal, dtype=torch.float32)  # Ensure X is a PyTorch tensor\n",
    "y_indices_train = np.array(y_indices_train)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "all_test_preds = []\n",
    "all_test_trues = []\n",
    "\n",
    "all_train_out = []\n",
    "all_test_out = []\n",
    "\n",
    "for fold_idx, (train_idx_out, test_idx_out) in enumerate(outer_cv.split(X_train_seasonal, y_indices_train)):\n",
    "    X_train_out, X_test_out = X_train_seasonal[train_idx_out], X_train_seasonal[test_idx_out]\n",
    "    y_train_out, y_test_out = y_train[train_idx_out], y_train[test_idx_out]\n",
    "    y_indices_test_out = y_indices_train[test_idx_out]\n",
    "\n",
    "    all_train_out.extend(train_idx_out.tolist())\n",
    "    all_test_out.extend(test_idx_out.tolist())\n",
    "\n",
    "    fold_ensemble_preds = []\n",
    "    for esb_idx in range(ensemble_per_fold):\n",
    "        torch.manual_seed(esb_idx)\n",
    "        np.random.seed(esb_idx)\n",
    "        random.seed(esb_idx)\n",
    "\n",
    "        def objective(trial):\n",
    "            lr = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "            wd = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "            dr = trial.suggest_float(\"dropout\", 0.2, 0.6)\n",
    "            out_c1 = trial.suggest_categorical(\"out_channels1\", [4, 8, 16])\n",
    "            out_c2 = trial.suggest_categorical(\"out_channels2\", [8, 16, 32])\n",
    "            ks = trial.suggest_categorical(\"kernel_size\", [3, 5, 7])\n",
    "            fc1 = trial.suggest_categorical(\"fc1_size\", [32, 64, 128])\n",
    "\n",
    "            inner_scores = []\n",
    "            for train_idx_in, val_idx_in in inner_cv.split(X_train_out, y_train_out.argmax(dim=1)):\n",
    "                X_train_fold = X_train_out[train_idx_in]\n",
    "                y_train_fold = y_train_out[train_idx_in]\n",
    "                X_val_fold = X_train_out[val_idx_in]\n",
    "                y_val_fold = y_train_out[val_idx_in]\n",
    "\n",
    "                model = CirculationTypeClassifier(num_classes, lat, lon, dropout_rate=dr, out_channels1=out_c1,\n",
    "                                                  out_channels2=out_c2, kernel_size=ks, fc1_size=fc1).to(device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                train_loader = DataLoader(TensorDataset(X_train_fold, y_train_fold), batch_size=batch_size, shuffle=True)\n",
    "                val_loader = DataLoader(TensorDataset(X_val_fold, y_val_fold), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                best_val_score = -np.inf\n",
    "                p_counter = 0\n",
    "\n",
    "                for epoch in range(epochs):\n",
    "                    _= model.train()\n",
    "                    for Xb, yb in train_loader:\n",
    "                        Xb, yb = Xb.to(device), yb.to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        output = model(Xb)\n",
    "                        loss = criterion(output, yb.argmax(dim=1))\n",
    "                        _= loss.backward()\n",
    "                        _= optimizer.step()\n",
    "\n",
    "                    _= model.eval()\n",
    "                    y_pred, y_true = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for Xb, yb in val_loader:\n",
    "                            Xb = Xb.to(device)\n",
    "                            output = model(Xb)\n",
    "                            y_pred.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "                            y_true.extend(yb.argmax(dim=1).cpu().numpy())\n",
    "                    val_score = balanced_accuracy_score(y_true, y_pred)\n",
    "                    if val_score > best_val_score:\n",
    "                        best_val_score = val_score\n",
    "                        p_counter = 0\n",
    "                    else:\n",
    "                        p_counter += 1\n",
    "                        if p_counter >= patience:\n",
    "                            break\n",
    "\n",
    "                inner_scores.append(best_val_score)\n",
    "            return np.mean(inner_scores)\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=esb_idx))\n",
    "        study.optimize(objective, n_trials=20)\n",
    "        best_hyperparams = study.best_params\n",
    "\n",
    "        # Save best model with optimal hyperparameters\n",
    "        best_model = CirculationTypeClassifier(num_classes, lat, lon,\n",
    "                                               dropout_rate=best_hyperparams['dropout'],\n",
    "                                               out_channels1=best_hyperparams['out_channels1'],\n",
    "                                               out_channels2=best_hyperparams['out_channels2'],\n",
    "                                               kernel_size=best_hyperparams['kernel_size'],\n",
    "                                               fc1_size=best_hyperparams['fc1_size']).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(best_model.parameters(),\n",
    "                               lr=best_hyperparams['learning_rate'],\n",
    "                               weight_decay=best_hyperparams['weight_decay'])\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        train_loader = DataLoader(TensorDataset(X_train_out, y_train_out), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            _= best_model.train()\n",
    "            for Xb, yb in train_loader:\n",
    "                Xb, yb = Xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = best_model(Xb)\n",
    "                loss = criterion(output, yb.argmax(dim=1))\n",
    "                _= loss.backward()\n",
    "                _= optimizer.step()\n",
    "\n",
    "        m_path = os.path.join(model_path, f\"model_f{fold_idx}_e{esb_idx}.pt\")\n",
    "        torch.save(best_model.state_dict(), m_path)\n",
    "        with open(os.path.join(model_path, f\"params_f{fold_idx}_e{esb_idx}.json\"), \"w\") as f:\n",
    "            json.dump(best_hyperparams, f, indent=2)\n",
    "\n",
    "        _= best_model.eval()\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in DataLoader(TensorDataset(X_test_out, y_test_out), batch_size=batch_size):\n",
    "                Xb = Xb.to(device)\n",
    "                outputs = best_model(Xb)\n",
    "                y_pred.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "        fold_ensemble_preds.append(y_pred)\n",
    "\n",
    "    fold_ensemble_preds = np.array(fold_ensemble_preds)\n",
    "    assert fold_ensemble_preds.shape == (ensemble_per_fold, X_test_out.shape[0]), \"fold_ensemble_preds must have correct dim!\"\n",
    "    fold_ensemble_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=fold_ensemble_preds)\n",
    "\n",
    "    all_test_preds.extend(fold_ensemble_preds.tolist())\n",
    "    all_test_trues.extend(y_indices_test_out.tolist())\n",
    "\n",
    "np.save(os.path.join(train_path, \"cvout_test_preds.npy\"), np.array(all_test_preds))\n",
    "np.save(os.path.join(train_path, \"cvout_test_trues.npy\"), np.array(all_test_trues))\n",
    "np.save(os.path.join(train_path, f\"train_idx_out.npy\"), all_train_out)\n",
    "np.save(os.path.join(train_path, f\"test_idx_out.npy\"), all_test_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
